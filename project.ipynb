{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: **Gauss-Newton Method** in **DFO-GN: Derivative-Free Nonlinear Least-Squares Solver**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the method\n",
    "\n",
    "Initially, we've used Newton's method as a rootfinding algorithm. However in the regression activity, we've learned that Newton's method can also be used for solving the optimization problem. In exploring non-linear regression, the Gauss-Newton method, a modification of Newton's method, is used to solve the nonlinear least squares problem, where it minimizes the sum of squared function values. The Gauss-Newton method is preferred over Newton's method because second derivatives that are challenging to compute are not required.\n",
    "\n",
    "## About the software\n",
    "\n",
    "Repository: https://github.com/numericalalgorithmsgroup/dfogn\n",
    "\n",
    "This software package implements the classical Gauss-Newton method as a derivative-free optimization algorithm designed to solve the nonlinear least squares problem:\n",
    "\n",
    "\\begin{split}\\min_{x\\in\\mathbb{R}^n}  &\\quad  f(x) := \\sum_{i=1}^{m}r_{i}(x)^2 \\\\\n",
    "\\text{s.t.} &\\quad  a \\leq x \\leq b\\end{split}\n",
    "\n",
    "This software was developed by Lindon Roberts and Carolina Cartis for their [research paper](https://link.springer.com/article/10.1007%2Fs12532-019-00161-7). The majority of methods that solve the nonlinear least squares problem involves a derivative-based algorithm. However, this derivative-free algorithm (DFO-GN) is preferred over a derivative-based algorithm if the residuals are noisy or if the residuals are expensive to evaluate. The software is developed in Python, is combined with SciPy and NumPy, and can be easily installed as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dfogn in /home/jovyan/.local/lib/python3.7/site-packages (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.11 in /opt/conda/lib/python3.7/site-packages (from dfogn) (1.18.1)\n",
      "Requirement already satisfied: scipy>=0.18 in /opt/conda/lib/python3.7/site-packages (from dfogn) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install dfogn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method as it appears in the software\n",
    "\n",
    "The main usage of DFO-GN is to use the following function *solve*:\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "soln = dfogn.solve(objfun, x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input *objfun* is a Python function which takes an input \n",
    "\n",
    "$$x\\in\\mathbb{R}^n$$\n",
    "\n",
    "where the input is a one-dimensional NumPy array. The input *x0* is the starting point for the solver and should be used as the best estimate to the solution\n",
    "\n",
    "$$x_{min}\\in\\mathbb{R}^n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples and Usage\n",
    "\n",
    "A common test problem is to minimize the [Rosenbrock function](https://numericalalgorithmsgroup.github.io/dfogn/build/html/userguide.html#how-to-use-dfo-gn), written as a least-squares problem and using common starting point $x_0=(-1.2,1)$.\n",
    "\n",
    "\\begin{split}\\min_{(x_1,x_2)\\in\\mathbb{R}^2}  &\\quad  [10(x_2-x_1^2)]^2 + [1-x_1]^2 \\\\\\end{split}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " *** DFO-GN results *** \n",
      "Solution xmin = [1. 1.]\n",
      "Objective value f(xmin) = 1.232595164e-30\n",
      "Needed 33 objective evaluations\n",
      "Residual vector = [-1.11022302e-15  0.00000000e+00]\n",
      "Approximate Jacobian = [[-2.00180000e+01  1.00000000e+01]\n",
      " [-1.00000000e+00  5.44052672e-15]]\n",
      "Exit flag = 0\n",
      "Success: Objective is sufficiently small\n"
     ]
    }
   ],
   "source": [
    "# DFO-GN example: minimize the Rosenbrock function\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import dfogn\n",
    "\n",
    "# Define the objective function\n",
    "def rosenbrock(x):\n",
    "    return np.array([10.0 * (x[1] - x[0] ** 2), 1.0 - x[0]])\n",
    "\n",
    "# Define the starting point\n",
    "x0 = np.array([-1.2, 1.0])\n",
    "\n",
    "# Call DFO-GN\n",
    "soln = dfogn.solve(rosenbrock, x0)\n",
    "\n",
    "# Display output\n",
    "print(\" *** DFO-GN results *** \")\n",
    "print(\"Solution xmin = %s\" % str(soln.x))\n",
    "print(\"Objective value f(xmin) = %.10g\" % soln.f)\n",
    "print(\"Needed %g objective evaluations\" % soln.nf)\n",
    "print(\"Residual vector = %s\" % str(soln.resid))\n",
    "print(\"Approximate Jacobian = %s\" % str(soln.jacobian))\n",
    "print(\"Exit flag = %g\" % soln.flag)\n",
    "print(soln.msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, DFO-GN is particularly effective when the residual (*objfun*) is noisy. Specifically, DFO-GN is preferred over derivative-based solvers such as SciPy. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demonstrate noise in function evaluation:\n",
      "objfun(x0) = [-4.4776183   2.20880346]\n",
      "objfun(x0) = [-4.44306447  2.24929965]\n",
      "objfun(x0) = [-4.48217255  2.17849989]\n",
      "objfun(x0) = [-4.44180389  2.19667014]\n",
      "objfun(x0) = [-4.39545837  2.20903317]\n",
      "\n",
      " *** DFO-GN results *** \n",
      "Solution xmin = [0.99999987 0.99999964]\n",
      "Objective value f(xmin) = 7.637183294e-13\n",
      "Needed 30 objective evaluations\n",
      "Residual vector = [-8.63477757e-07  1.34627233e-07]\n",
      "Approximate Jacobian = [[-1.96795283e+01  9.91949566e+00]\n",
      " [-1.00447169e+00  5.59078195e-04]]\n",
      "Exit flag = 0\n",
      "Success: Objective is sufficiently small\n",
      "\n",
      " *** SciPy results *** \n",
      "Solution xmin = [-1.19999887  1.00000341]\n",
      "Objective value f(xmin) = 23.80789053\n",
      "Needed 6 objective evaluations\n",
      "Exit flag = 3\n",
      "`xtol` termination condition is satisfied.\n"
     ]
    }
   ],
   "source": [
    "# DFO-GN example: minimize the Rosenbrock function\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import dfogn\n",
    "\n",
    "# Define the objective function\n",
    "def rosenbrock(x):\n",
    "    return np.array([10.0 * (x[1] - x[0] ** 2), 1.0 - x[0]])\n",
    "\n",
    "# Modified objective function: add 1% Gaussian noise\n",
    "def rosenbrock_noisy(x):\n",
    "    return rosenbrock(x) * (1.0 + 1e-2 * np.random.normal(size=(2,)))\n",
    "\n",
    "# Define the starting point\n",
    "x0 = np.array([-1.2, 1.0])\n",
    "\n",
    "# Set random seed (for reproducibility)\n",
    "np.random.seed(0)\n",
    "\n",
    "print(\"Demonstrate noise in function evaluation:\")\n",
    "for i in range(5):\n",
    "    print(\"objfun(x0) = %s\" % str(rosenbrock_noisy(x0)))\n",
    "print(\"\")\n",
    "\n",
    "# Call DFO-GN\n",
    "soln = dfogn.solve(rosenbrock_noisy, x0)\n",
    "\n",
    "# Display output\n",
    "print(\" *** DFO-GN results *** \")\n",
    "print(\"Solution xmin = %s\" % str(soln.x))\n",
    "print(\"Objective value f(xmin) = %.10g\" % soln.f)\n",
    "print(\"Needed %g objective evaluations\" % soln.nf)\n",
    "print(\"Residual vector = %s\" % str(soln.resid))\n",
    "print(\"Approximate Jacobian = %s\" % str(soln.jacobian))\n",
    "print(\"Exit flag = %g\" % soln.flag)\n",
    "print(soln.msg)\n",
    "\n",
    "# Compare with a derivative-based solver\n",
    "import scipy.optimize as opt\n",
    "soln = opt.least_squares(rosenbrock_noisy, x0)\n",
    "\n",
    "print(\"\")\n",
    "print(\" *** SciPy results *** \")\n",
    "print(\"Solution xmin = %s\" % str(soln.x))\n",
    "print(\"Objective value f(xmin) = %.10g\" % (2.0 * soln.cost))\n",
    "print(\"Needed %g objective evaluations\" % soln.nfev)\n",
    "print(\"Exit flag = %g\" % soln.status)\n",
    "print(soln.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example demostrates why DFO-GN is preferred over derivative-based solvers such as SciPy shown by the results above. SciPy's derivative based solver is unable to make any progress when the residual is noisy while DFO-GN can.\n",
    "\n",
    "Another example of DFO-GN's usage is solving a nonlinear system of equations. From [here](https://support.sas.com/documentation/cdl/en/imlug/66112/HTML/default/viewer.htm#imlug_genstatexpls_sect004.htm), the following set of equations are: \n",
    "\n",
    "\\begin{split}x_1 + x_2 - x_1 x_2 + 2 &= 0, \\\\\n",
    "x_1 \\exp(-x_2) - 1 &= 0.\\end{split}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " *** DFO-GN results *** \n",
      "Solution xmin = [ 0.09777309 -2.32510588]\n",
      "Objective value f(xmin) = 2.916172822e-16\n",
      "Needed 13 objective evaluations\n",
      "Residual vector = [-1.38601752e-09 -1.70204653e-08]\n",
      "Exit flag = 0\n",
      "Success: Objective is sufficiently small\n"
     ]
    }
   ],
   "source": [
    "# DFO-GN example: Solving a nonlinear system of equations\n",
    "# http://support.sas.com/documentation/cdl/en/imlug/66112/HTML/default/viewer.htm#imlug_genstatexpls_sect004.htm\n",
    "\n",
    "from __future__ import print_function\n",
    "import math\n",
    "import numpy as np\n",
    "import dfogn\n",
    "\n",
    "# Want to solve:\n",
    "#   x1 + x2 - x1*x2 + 2 = 0\n",
    "#   x1 * exp(-x2) - 1   = 0\n",
    "def nonlinear_system(x):\n",
    "    return np.array([x[0] + x[1] - x[0]*x[1] + 2,\n",
    "                     x[0] * math.exp(-x[1]) - 1.0])\n",
    "\n",
    "# Warning: if there are multiple solutions, which one\n",
    "#          DFO-GN returns will likely depend on x0!\n",
    "x0 = np.array([0.1, -2.0])\n",
    "\n",
    "soln = dfogn.solve(nonlinear_system, x0)\n",
    "\n",
    "# Display output\n",
    "print(\" *** DFO-GN results *** \")\n",
    "print(\"Solution xmin = %s\" % str(soln.x))\n",
    "print(\"Objective value f(xmin) = %.10g\" % soln.f)\n",
    "print(\"Needed %g objective evaluations\" % soln.nf)\n",
    "print(\"Residual vector = %s\" % str(soln.resid))\n",
    "print(\"Exit flag = %g\" % soln.flag)\n",
    "print(soln.msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the entries of the residual vector are very small indicating that the two equations are solved to a high accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open questions\n",
    "\n",
    "* The authors of this software suggest that derivative-free optimization algorithms (DFO-GN) are preferred over derivative-based algorithms only in a few situations, otherwise use the derivative-based algorithms. Then, derivative-free optimization algorithms fills a convenience that derivative-based algorithms fail to solve. Is this a fair statement? Will we see more potential use case for the derivative-free optimization algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
